{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:40:13.831520Z",
     "start_time": "2020-01-11T00:40:10.337542Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import date, datetime, timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:40:53.590303Z",
     "start_time": "2020-01-11T00:40:13.835802Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\n",
    "test = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\n",
    "building_meta = pd.read_csv('../input/building_metadata.csv')\n",
    "weather_train = pd.read_csv('../input/weather_train.csv', parse_dates=['timestamp'])\n",
    "weather_test = pd.read_csv('../input/weather_test.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:41:49.809364Z",
     "start_time": "2020-01-11T00:40:53.592611Z"
    }
   },
   "outputs": [],
   "source": [
    "# 覚え書き\n",
    "# 連続で同じ値を取るやつを除去\n",
    "# ただし、同じ値を取るやつが最小値だった場合は除去しない(電気データの場合、最小値=休みの日とかの可能性があるため)\n",
    "\n",
    "del_list = list()\n",
    "\n",
    "for building_id in range(1449):\n",
    "    train_gb = train[train['building_id'] == building_id].groupby(\"meter\")\n",
    "\n",
    "    for meter, tmp_df in train_gb:\n",
    "#         print(\"building_id: {}, meter: {}\".format(building_id, meter))\n",
    "        data = tmp_df['meter_reading'].values\n",
    "#         splited_value = np.split(data, np.where((data[1:] != data[:-1]) | (data[1:] == min(data)))[0] + 1)\n",
    "#         splited_date = np.split(tmp_df.timestamp.values, np.where((data[1:] != data[:-1]) | (data[1:] == min(data)))[0] + 1)\n",
    "        splited_idx = np.split(tmp_df.index.values, np.where((data[1:] != data[:-1]) | (data[1:] == min(data)))[0] + 1)\n",
    "        for i, x in enumerate(splited_idx):\n",
    "            if len(x) > 24:\n",
    "#                 print(\"length: {},\\t{}-{},\\tvalue: {}\".format(len(x), x[0], x[-1], splited_value[i][0]))\n",
    "                del_list.extend(x[1:])\n",
    "                \n",
    "                \n",
    "#         print()\n",
    "\n",
    "del tmp_df, train_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:42:03.658859Z",
     "start_time": "2020-01-11T00:41:49.811339Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/dirac/sano/env36/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/mnt/dirac/sano/env36/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/mnt/dirac/sano/env36/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/mnt/dirac/sano/env36/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/mnt/dirac/sano/env36/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/mnt/dirac/sano/env36/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def idx_to_drop(df):\n",
    "    drop_cols = []\n",
    "    electric_zero = df[(df['meter']==0)&(df['meter_reading']==0)].index.values.tolist()\n",
    "    drop_cols.extend(electric_zero)\n",
    "    not_summer = df[(df['timestamp'].dt.month!=7)&(df['timestamp'].dt.month!=8)]\n",
    "    not_summer['cumsum'] = not_summer.groupby(['building_id','meter'])['meter_reading'].cumsum()\n",
    "    not_summer['shifted'] = not_summer.groupby(['building_id','meter'])['cumsum'].shift(48)\n",
    "    not_summer['difference'] = not_summer['cumsum']-not_summer['shifted']\n",
    "    steam_zero = not_summer[(not_summer['difference']==0) & (not_summer['meter']==2)].index.values.tolist()\n",
    "    hotwater_zero = not_summer[(not_summer['difference']==0) & (not_summer['meter']==3)].index.values.tolist()\n",
    "    drop_cols.extend(steam_zero)\n",
    "    drop_cols.extend(hotwater_zero)\n",
    "    del not_summer\n",
    "    not_winter = train[(df['timestamp'].dt.month!=12)&(df['timestamp'].dt.month!=1)]\n",
    "    not_winter['cumsum'] = not_winter.groupby(['building_id','meter'])['meter_reading'].cumsum()\n",
    "    not_winter['shifted'] = not_winter.groupby(['building_id','meter'])['cumsum'].shift(48)\n",
    "    not_winter['difference'] = not_winter['cumsum']-not_winter['shifted']\n",
    "    chilled_zero = not_winter[(not_winter['difference']==0) & (not_winter['meter']==1)].index.values.tolist()\n",
    "    drop_cols.extend(chilled_zero)\n",
    "    return drop_cols\n",
    "\n",
    "del_list.extend(idx_to_drop(train))\n",
    "\n",
    "\n",
    "\n",
    "del_list_new = train.loc[del_list].index#query('timestamp < 20160901').index\n",
    "\n",
    "# 行の削除\n",
    "train = train.drop(del_list_new)\n",
    "\n",
    "train = train.query('(not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")) & (not (meter==0 & meter_reading==0))')\n",
    "train['meter_reading'] = np.log1p(train['meter_reading'])\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:42:03.796439Z",
     "start_time": "2020-01-11T00:42:03.660704Z"
    }
   },
   "outputs": [],
   "source": [
    "weather = pd.concat([weather_train, weather_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "# dataframeの定義\n",
    "country = ['UnitedStates', 'England', 'UnitedStates', 'UnitedStates', 'UnitedStates',\n",
    "           'England', 'UnitedStates', 'Canada', 'UnitedStates', 'UnitedStates',\n",
    "           'UnitedStates', 'Canada', 'Ireland', 'UnitedStates', 'UnitedStates', 'UnitedStates']\n",
    "\n",
    "city = ['Jacksonville', 'London', 'Phoenix', 'Philadelphia', 'San Francisco',\n",
    "       'Loughborough', 'Philadelphia', 'Montreal', 'Jacksonville', 'San Antonio',\n",
    "       'Las Vegas', 'Montreal', 'Dublin', 'Minneapolis', 'Philadelphia', 'Pittsburgh']\n",
    "\n",
    "UTC_offset = [-4, 0, -7, -4, -9, 0, -4, -4, -4, -5, -7, -4, 0, -5, -4, -4]\n",
    "\n",
    "location_data = pd.DataFrame(np.array([country, city, UTC_offset]).T, index=range(16), columns=['country', 'city', 'UTC_offset'])\n",
    "\n",
    "\n",
    "# timestampの補正\n",
    "for idx in location_data.index:\n",
    "    weather.loc[weather['site_id']==idx, 'timestamp'] += timedelta(hours=int(location_data.loc[idx, 'UTC_offset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:43:47.764782Z",
     "start_time": "2020-01-11T00:42:03.798175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/dirac/sano/env36/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def fill_weather_dataset(weather_df):\n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.strptime(weather_df['timestamp'].min(),time_format)\n",
    "    end_date = datetime.strptime(weather_df['timestamp'].max(),time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_df = pd.concat([weather_df,new_rows])\n",
    "\n",
    "        weather_df = weather_df.reset_index(drop=True)           \n",
    "\n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    # Step 2\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "    weather_df.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather_df.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather_df.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n",
    "        \n",
    "    return weather_df\n",
    "\n",
    "weather['timestamp'] = weather['timestamp'].astype(str)\n",
    "weather = fill_weather_dataset(weather)\n",
    "weather['timestamp'] = pd.to_datetime(weather['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:43:49.003279Z",
     "start_time": "2020-01-11T00:43:47.767232Z"
    }
   },
   "outputs": [],
   "source": [
    "### 休日情報\n",
    "\n",
    "import holidays\n",
    "\n",
    "en_holidays = holidays.England()\n",
    "ir_holidays = holidays.Ireland()\n",
    "ca_holidays = holidays.Canada()\n",
    "us_holidays = holidays.UnitedStates()\n",
    "\n",
    "en_idx = weather.query('site_id == 1 or site_id == 5').index\n",
    "ir_idx = weather.query('site_id == 12').index\n",
    "ca_idx = weather.query('site_id == 7 or site_id == 11').index\n",
    "us_idx = weather.query('site_id == 0 or site_id == 2 or site_id == 3 or site_id == 4 or site_id == 6 or site_id == 8 or site_id == 9 or site_id == 10 or site_id == 13 or site_id == 14 or site_id == 15').index\n",
    "\n",
    "weather['IsHoliday'] = 0\n",
    "weather.loc[en_idx, 'IsHoliday'] = weather.loc[en_idx, 'timestamp'].apply(lambda x: en_holidays.get(x, default=0))\n",
    "weather.loc[ir_idx, 'IsHoliday'] = weather.loc[ir_idx, 'timestamp'].apply(lambda x: ir_holidays.get(x, default=0))\n",
    "weather.loc[ca_idx, 'IsHoliday'] = weather.loc[ca_idx, 'timestamp'].apply(lambda x: ca_holidays.get(x, default=0))\n",
    "weather.loc[us_idx, 'IsHoliday'] = weather.loc[us_idx, 'timestamp'].apply(lambda x: us_holidays.get(x, default=0))\n",
    "\n",
    "holiday_idx = weather['IsHoliday'] != 0\n",
    "weather.loc[holiday_idx, 'IsHoliday'] = 1\n",
    "weather['IsHoliday'] = weather['IsHoliday'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:44:28.924944Z",
     "start_time": "2020-01-11T00:43:49.005121Z"
    }
   },
   "outputs": [],
   "source": [
    "target = train['meter_reading'].values\n",
    "# train = train.drop('meter_reading', axis=1)\n",
    "row_id = test['row_id']\n",
    "test = test.drop('row_id', axis=1)\n",
    "\n",
    "df = pd.concat([train.drop('meter_reading', axis=1), test], axis=0).reset_index(drop=True)\n",
    "df = df.merge(building_meta, on='building_id', how='left')\n",
    "\n",
    "df = df.merge(weather, on=['site_id', 'timestamp'], how='left')\n",
    "\n",
    "df['day'] = df['timestamp'].dt.day #// 3\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "train = df.iloc[:len(target)].copy().reset_index(drop=True)\n",
    "train['meter_reading'] = target#.values\n",
    "test = df.iloc[len(target):].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:44:29.112994Z",
     "start_time": "2020-01-11T00:44:28.929186Z"
    }
   },
   "outputs": [],
   "source": [
    "df['is_day_off_or_holiday'] = (df['weekday'] >= 5) | df['IsHoliday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:44:39.794214Z",
     "start_time": "2020-01-11T00:44:29.114878Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_fraction(col1, col2):\n",
    "    col2_frac = train.groupby([col1, col2])[['meter_reading']].median()\n",
    "    col2_frac_idx = col2_frac.index\n",
    "    col2_frac_sum = col2_frac.groupby(col1).sum().rename(columns = {'meter_reading':'sum'})\n",
    "    col2_frac = col2_frac.merge(col2_frac_sum, on = col1, how='left')\n",
    "    col2_frac.index = col2_frac_idx\n",
    "    col2_frac['frac_{}_{}'.format(col1, col2)] = col2_frac['meter_reading'] / col2_frac['sum']\n",
    "    col2_frac = col2_frac[['frac_{}_{}'.format(col1, col2)]]\n",
    "    return col2_frac\n",
    "\n",
    "\n",
    "building_weekday_frac = make_fraction('building_id', 'weekday')\n",
    "building_hour_frac = make_fraction('building_id', 'hour')\n",
    "building_day_frac = make_fraction('building_id', 'day')\n",
    "\n",
    "primary_weekday_frac = make_fraction('primary_use', 'weekday')\n",
    "primary_hour_frac = make_fraction('primary_use', 'hour')\n",
    "primary_day_frac = make_fraction('primary_use', 'day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:46:07.194464Z",
     "start_time": "2020-01-11T00:44:39.799474Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.merge(building_weekday_frac, on=['building_id', 'weekday'], how='left')\n",
    "df = df.merge(building_hour_frac, on=['building_id', 'hour'], how='left')\n",
    "df = df.merge(building_day_frac, on=['building_id', 'day'], how='left')\n",
    "\n",
    "df = df.merge(primary_weekday_frac, on=['primary_use', 'weekday'], how='left')\n",
    "df = df.merge(primary_hour_frac, on=['primary_use', 'hour'], how='left')\n",
    "df = df.merge(primary_day_frac, on=['primary_use', 'day'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:46:07.203337Z",
     "start_time": "2020-01-11T00:46:07.199143Z"
    }
   },
   "outputs": [],
   "source": [
    "# median\n",
    "\n",
    "# df['median_building_id_weekday'] = train.groupby(['building_id', 'weekday'])['meter_reading'].transform('median')\n",
    "# df['median_building_id_hour'] = train.groupby(['building_id', 'hour'])['meter_reading'].transform('median')\n",
    "# df['median_building_id_day'] = train.groupby(['building_id', 'day'])['meter_reading'].transform('median')\n",
    "# df['median_primary_use_weekday'] = train.groupby(['primary_use', 'weekday'])['meter_reading'].transform('median')\n",
    "# df['median_primary_use_hour'] = train.groupby(['primary_use', 'hour'])['meter_reading'].transform('median')\n",
    "# df['median_primary_use_day'] = train.groupby(['primary_use', 'day'])['meter_reading'].transform('median')\n",
    "\n",
    "# df = df.drop(['median_building_id_weekday',\n",
    "#       'median_building_id_hour',\n",
    "#       'median_building_id_day',\n",
    "#       'median_primary_use_weekday',\n",
    "#       'median_primary_use_hour',\n",
    "#       'median_primary_use_day'], axis=1)\n",
    "\n",
    "\n",
    "# * wind_direction(frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:46:45.917605Z",
     "start_time": "2020-01-11T00:46:07.205649Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 建物ごとの平均\n",
    "# building_meter_average = train.groupby(['building_id', 'meter'])['meter_reading'].mean().rename('building_meter_average')\n",
    "# df = df.merge(building_meter_average, on=['building_id', 'meter'], how='left')\n",
    "\n",
    "\n",
    "# 建物ごとの分位点(95パーセンタイル)\n",
    "building_meter_95 = train.groupby(['building_id', 'meter'])['meter_reading'].apply(lambda arr: np.percentile(arr, 95)).rename('building_meter_95')\n",
    "df = df.merge(building_meter_95, on=['building_id', 'meter'], how='left')\n",
    "\n",
    "# 建物ごとの分位点(5パーセンタイル)\n",
    "building_meter_5 = train.groupby(['building_id', 'meter'])['meter_reading'].apply(lambda arr: np.percentile(arr, 5)).rename('building_meter_5')\n",
    "df = df.merge(building_meter_5, on=['building_id', 'meter'], how='left')\n",
    "\n",
    "\n",
    "\n",
    "# # minmaxscalingして予測したい場合\n",
    "# building_meter_95 = train.groupby(['building_id', 'meter'])['meter_reading'].apply(lambda arr: np.percentile(arr, 95)).rename('building_meter_95')\n",
    "# building_meter_95 += 0.5\n",
    "# train = train.merge(building_meter_95, on=['building_id', 'meter'], how='left')\n",
    "\n",
    "# train['meter_reading'] /= train['building_meter_95']\n",
    "# target = train['meter_reading'].values\n",
    "\n",
    "\n",
    "# ### 一部属性をカテゴリカル変数に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:47:08.757386Z",
     "start_time": "2020-01-11T00:46:45.920263Z"
    }
   },
   "outputs": [],
   "source": [
    "is_categorical = ['meter', 'building_id', 'site_id', 'primary_use', 'hour', 'day', 'weekday']\n",
    "df[is_categorical] = df[is_categorical].astype('category')\n",
    "df['year_built_cat'] = df['year_built'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:47:13.126781Z",
     "start_time": "2020-01-11T00:47:08.760657Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_columns = []#, 'hour', 'day', 'weekday']\n",
    "drop_df = df[drop_columns]\n",
    "df = df.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:47:13.132455Z",
     "start_time": "2020-01-11T00:47:13.129392Z"
    }
   },
   "outputs": [],
   "source": [
    "# train = df.iloc[:len(target)].copy().reset_index(drop=True)\n",
    "# train['meter_reading'] = target#.values\n",
    "# df = df.merge(train.groupby(['building_id','weekday'])['meter_reading'].agg(['mean', 'median']), on=['building_id','weekday'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:47:21.912219Z",
     "start_time": "2020-01-11T00:47:13.135593Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fe = df.iloc[:len(target)].copy().reset_index(drop=True)\n",
    "train_fe['meter_reading'] = target#.values\n",
    "test_fe = df.iloc[len(target):].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:47:21.918819Z",
     "start_time": "2020-01-11T00:47:21.915589Z"
    }
   },
   "outputs": [],
   "source": [
    "# target_fe = train_fe['meter_reading']\n",
    "# train_fe = train_fe.drop('meter_reading', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:47:22.005722Z",
     "start_time": "2020-01-11T00:47:21.921955Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_fe_all = df.iloc[:len(target)].copy()\n",
    "# train_fe_all['meter_reading'] = target#.values\n",
    "\n",
    "# test_fe_all = df.iloc[len(target):].copy()\n",
    "# test_fe_all['row_id'] = row_id.values\n",
    "\n",
    "# with open('../input/train_fe_all.zip', 'wb') as f:\n",
    "#     pickle.dump(train_fe_all, f)\n",
    "\n",
    "# with open('../input/test_fe_all_2017.zip', 'wb') as f:\n",
    "#     pickle.dump(test_fe_all.query('timestamp < 20180101'), f)\n",
    "    \n",
    "# with open('../input/test_fe_all_2018.zip', 'wb') as f:\n",
    "#     pickle.dump(test_fe_all.query('20180101 <= timestamp'), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T00:47:45.877931Z",
     "start_time": "2020-01-11T00:47:40.306439Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fe.to_feather('../prepare_data/train_fe.ftr')\n",
    "test_fe.to_feather('../prepare_data/test_fe.ftr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
